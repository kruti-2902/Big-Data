{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5186513e-e0c5-4af3-968c-94ecf864f173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (3.5.1)\r\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f4fde3-5275-465a-9cc7-957acc3d061e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (2.3.0)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed76fe8b-723f-45fe-a76b-6994a8474bb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'group.id': 'inventory_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "inventory_consumer = Consumer(consumer_config)\n",
    "inventory_consumer.subscribe(['inventory_orders'])\n",
    "\n",
    "def handle_inventory_data(order):\n",
    "    # Process inventory data here\n",
    "    print(\"Processing inventory data:\", order)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = inventory_consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                continue\n",
    "            else:\n",
    "                print(msg.error())\n",
    "                break\n",
    "        order = json.loads(msg.value().decode('utf-8'))\n",
    "        if order['type'] == 'inventory':\n",
    "            handle_inventory_data(order)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "finally:\n",
    "    inventory_consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05620b4d-5ba8-44c6-84eb-68ebc29a8be3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delivery_consumer = Consumer(consumer_config)\n",
    "delivery_consumer.subscribe(['delivery_orders'])\n",
    "\n",
    "# Function to handle delivery data\n",
    "def handle_delivery_data(order):\n",
    "    print(\"Processing delivery data:\", order)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = delivery_consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                continue\n",
    "            else:\n",
    "                print(msg.error())\n",
    "                break\n",
    "        order = json.loads(msg.value().decode('utf-8'))\n",
    "        if order['type'] == 'delivery':\n",
    "            handle_delivery_data(order)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "finally:\n",
    "    delivery_consumer.close()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "202318032_Kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
