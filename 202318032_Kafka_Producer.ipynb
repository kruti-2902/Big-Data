{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5186513e-e0c5-4af3-968c-94ecf864f173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (3.5.1)\r\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f4fde3-5275-465a-9cc7-957acc3d061e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/lib/python3.9/site-packages (2.3.0)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d48c73d8-d4ce-4bd9-a12d-0f7858dad4bb/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eaa64fb-d51b-4381-9166-5a9103be3b0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 0"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "file_path = '/FileStore/shared_uploads/202318032@daiict.ac.in/data-2.json'\n",
    "df = spark.read.json(file_path)\n",
    "\n",
    "# Define Kafka producer configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers\n",
    "}\n",
    "inventory_producer = Producer(producer_config)\n",
    "\n",
    "# Function to produce a message for inventory orders\n",
    "def produce_inventory_order(order):\n",
    "    message = json.dumps(order)\n",
    "    inventory_producer.produce('inventory_orders', message.encode('utf-8'))\n",
    "\n",
    "# Assuming `df` is the DataFrame containing orders\n",
    "for row in df.collect():\n",
    "    order = row.asDict()\n",
    "    if order['type'] == 'inventory':\n",
    "        produce_inventory_order(order)\n",
    "\n",
    "# Flush producer to ensure all messages are sent\n",
    "inventory_producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697269cd-f241-4722-9aa6-091d43c1705b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 0"
     ]
    }
   ],
   "source": [
    "# Create Kafka producer for delivery orders\n",
    "delivery_producer = Producer(producer_config)\n",
    "\n",
    "# Function to produce a message for delivery orders\n",
    "def produce_delivery_order(order):\n",
    "    message = json.dumps(order)\n",
    "    delivery_producer.produce('delivery_orders', message.encode('utf-8'))\n",
    "\n",
    "for row in df.collect():\n",
    "    order = row.asDict()\n",
    "    if order['type'] == 'delivery':\n",
    "        produce_delivery_order(order)\n",
    "\n",
    "# Flush producer to ensure all messages are sent\n",
    "delivery_producer.flush()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "202318032_Kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
